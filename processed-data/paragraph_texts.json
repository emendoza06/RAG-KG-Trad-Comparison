["= Introduction to Neo4j & GenAI\n:order: 1\n:type: lesson\n\nThis course will get you started learning about using Neo4j with Generative AI.\n\nYou will learn about:\n\n* Large Language Models (LLMs)\n* Knowledge Graphs\n* How to use Neo4j for Retrieval Augmented Generation (RAG)\n* Integrating Neo4j and LLMs using Python and Langchain\n\nFirst, let's cover the basics.\n\n== What is Neo4j?\n\nNeo4j is a graph database and analytics system that allows us to store, manage, and query highly connected data.\n\nUnlike traditional relational databases, which use tables and rows, Neo4j uses a graph-based model with nodes and relationships.\n\nimage::images/large-social-graph.jpg[A visualization of a graph showing nodes connected by relationships]\n\nMaking Neo4j particularly well-suited for representing and querying complex, interconnected data.\n\n[TIP]\n.New to Neo4j?\nTo learn about graph databases and Neo4j, check out the  link:/courses/neo4j-fundamentals/[GraphAcademy Neo4j Fundamentals course^].\n\n== What are Knowledge Graphs?\n\nKnowledge graphs are a specific implementation of a Graph Database, where information is captured and integrated from many different sources, representing the inherent knowledge of a particular domain.\n\nThey provide a structured way to represent entities, their attributes, and their relationships, allowing for a comprehensive and interconnected understanding of the information within that domain.", "They provide a structured way to represent entities, their attributes, and their relationships, allowing for a comprehensive and interconnected understanding of the information within that domain.\n\nKnowledge graphs break down sources of information and integrate them, allowing you to see the relationships between the data.\n\nimage::images/generic-knowledge-graph.svg[a diagram of an abstract knowledge graph showing how sources contain chunks of data about topics which can be related to other topics]\n\nYou can tailor knowledge graphs for semantic search, data retrieval, and reasoning. \n\nYou may not be familiar with the term knowledge graph, but you have probably used one. Search engines typically use knowledge graphs to provide information about people, places, and things.\n\nThe following knowledge graph could represent Neo4j:\n\nimage::images/neo4j-google-knowledge-graph.svg[An example of a knowledge graph of Neo4j showing the relationships between people, places, and things]\n\nThis integration from diverse sources gives knowledge graphs a more holistic view and facilitates complex queries, analytics, and insights.\n\n[TIP]\n.Knowledge Graphs and Ontologies\nFor more on Knowledge Graphs, Ontologies, we recommend watching the\nlink:https://www.youtube.com/watch?v=NQqWBnyQlS4&list=PL9Hl4pk2FsvX-5QPvwChB-ni_mFF97rCE[Going Meta \u00e2\u20ac\u201c A Series on Graphs, Semantics and Knowledge series on YouTube^].", "Knowledge graphs can readily adapt and evolve as they grow, taking on new information and structure changes.\n\n== Large Language Models & Generative AI\n\nLarge Language Models, referred to as LLMs, learn the underlying structure and distribution of the data and can then generate new samples that resemble the original data.\n\nLLMs are trained on vast amounts of text data to understand and generate human-like text. LLMs can answer questions, create content, and assist with various linguistic tasks by leveraging patterns learned from the data.\n\nGenerative AI is a class of algorithms and models that can generate new content, such as images, text, or even music. New content is generated based on user prompting, existing patterns, and examples from existing data.\n\n=== Instructing an LLM\n\nThe response generated by an LLM is a probabilistic continuation of the instructions it receives. The LLM provides the most likely response based on the patterns it has learned from its training data.\n\nIn simple terms, if presented with the prompt _\"Continue this sequence - A B C\"_, an LLM could respond _\"D E F\"_.\n\nTo get an LLM to perform a task, you provide a **prompt**, a piece of text that should specify your requirements and provide clear instructions on how to respond.\n\nimage::images/llm-prompt-interaction.svg[A user asks an LLM the question 'What is an LLM? Give the response using simple language avoiding jargon.', the LLM responds with a simple definition of an LLM.]", "Precision in the task description, potentially combined with examples or context, ensures that the model understands the intent and produces relevant and accurate outputs.\n\nAn example prompt may be a simple question.\n\n    What is the capital of Japan?\n\nOr, it could be more descriptive. For example:\n\n    Tell me about the capital of Japan.\n    Produce a brief list of talking points exploring its culture and history.\n    The content should be targeted at tourists.\n    Your readers may have English as a second language, so use simple terms and avoid colloquialisms.\n    Avoid Jargon at all costs.\n    Return the results as a list of JSON strings containing content formatted in Markdown.\n\nThe LLM will interpret these instructions and return a response based on the patterns it has learned from its training data.\n\n== Potential Problems\n\nWhile LLMs provide a lot of potential, you should also be cautious.\n\nAt their core, LLMs are trained to predict the following word(s) in a sequence.\n\nThe words are based on the patterns and relationships from other text in the training data. The sources for this training data are often the internet, books, and other publicly available text. This data could be of questionable quality and maybe be incorrect. Training happens at a point in time, it may not reflect the current state of the world and would not include any private information.", "LLMs are fine-tuned to be as helpful as possible, even if that means occasionally generating misleading or baseless content, a phenomenon known as **hallucination**.\n\nFor example, when asked to _\"Describe the moon.\"_ and LLM may respond with _\"The moon is made of cheese.\"_. While this is a common saying, it is not true.\n\nimage::images/confused-llm.svg[A diagram of a confused LLM with a question mark thinking about the moon and cheese.]\n\nWhile LLMs can represent the essence of words and phrases, they don't possess a genuine understanding or ethical judgment of the content.\n\nThese factors can lead to outputs that might be biased, devoid of context, or lack logical coherence.\n\n== Fixing Hallucinations\n\nProviding additional *contextual* data helps to _ground_ the LLM's responses and make them more accurate.\n\nA knowledge graph is a mechanism for providing additional data to an LLM. Data within the knowledge graph can guide the LLM to provide more relevant, accurate, and reliable responses. \n\nWhile the LLM uses its language skills to interpret and respond to the contextual data, it will not disregard the original training data.\n\nYou can think of the original training data as the base knowledge and linguistic capabilities, while the contextual information guides in specific situations.\n\nThe combination of both approaches enables the LLM to generate more meaningful responses.", "The combination of both approaches enables the LLM to generate more meaningful responses.\n\nThroughout this course, you will explore how to leverage the capabilities of Neo4j and Generative AI to build intelligent, context-aware systems.\n\nYou will apply the information and skills learned in the course to build an engine that provides recommendations and information about movies and people.\n\n== Check Your Understanding\n\ninclude::questions/1-hallucination.adoc[leveloffset=+1]\ninclude::questions/2-fixing-hallucination.adoc[leveloffset=+1]\n\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you learned about LLMs, their benefits and challenges.\n\nIn the next lesson, you will learn about hallucination and the strategies for avoiding it.", "= Avoiding Hallucination\n:order: 2\n:type: lesson\n\nAs you learned in the previous lesson, LLMs can \"make things up\".\n\nLLMs are designed to generate human-like text based on the patterns they've identified in vast amounts of data. \n\nDue to their reliance on patterns and the sheer volume of training information, LLMs sometimes **hallucinate** or produce outputs that manifest as generating untrue facts, asserting details with unwarranted confidence, or crafting plausible yet nonsensical explanations.\n\nThese manifestations arise from a mix of _overfitting_, biases in the training data, and the model's attempt to generalize from vast amounts of information.\n\n== Common Hallucination Problems\n\nLet's take a closer look at some reasons why this may occur.\n\n=== Temperature\n\nLLMs have a _temperature_, corresponding to the amount of randomness the underlying model should use when generating the text.\n\nThe higher the temperature value, the more random the generated result will become, and the more likely the response will contain false statements.\n\nA higher temperature may be appropriate when configuring an LLM to respond with more diverse and creative outputs, but it comes at the expense of consistency and precision.\n\nFor example, a higher temperature may be suitable for constructing a work of fiction or a novel joke.\n\nOn the other hand, a lower temperature, even `0`, is required when a response grounded in facts is essential.", "On the other hand, a lower temperature, even `0`, is required when a response grounded in facts is essential.\n\n[TIP]\n.Consider the correct temperature\n====\nIn June 2023, link:https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/[A US judged sanctioned two US lawyers for submitting an LLM-generated legal brief^] that contained six fictitious case citations.\n====\n\nA quick fix _may_ be to reduce the temperature. But more likely, the LLM is hallucinating because it hasn't got the information required.\n\n=== Missing Information\n\nThe training process for LLMs is intricate and time-intensive, often requiring vast datasets compiled over extended periods. As such, these models might lack the most recent information or might miss out on specific niche topics not well-represented in their training data.\n\nFor instance, if an LLM's last update were in September 2022, it would be unaware of world events or advancements in various fields that occurred post that date, leading to potential gaps in its knowledge or responses that seem out of touch with current realities.\n\nIf the user asks a question on information that is hard to find or outside of the public domain, it will be virtually impossible for an LLM to respond accurately.\n\nLuckily, this is where factual information from data sources such as knowledge graphs can help.\n\n=== Model Training and Complexity", "Luckily, this is where factual information from data sources such as knowledge graphs can help.\n\n=== Model Training and Complexity\n\nLarge Language Models (LLMs) are often considered \"black boxes\" due to the difficulty deciphering their decision-making processes.\n\nimage::images/llm-blackbox.svg[An LLM as a black box, responding to the question 'How did you determine that answer?' with 'I don't know.']\n\nThe complexity of these models, combined with potential training on erroneous or misleading data, means that their outputs can sometimes be unpredictable or inaccurate.\n\nFor example, an LLM might produce a biased or incorrect answer when asked about a controversial historical event. \n\nFurthermore, it would be near impossible to trace back how the model arrived at that conclusion. The LLM would also be unable to provide the sources for its output or explain its reasoning.\n\n== Improving LLM Accuracy\n\nThe following methods can be employed to help guide LLMs to produce more consistent and accurate results.\n\n=== Prompt Engineering\n\nPrompt engineering is developing specific and deliberate instructions that guide the LLM toward the desired response.\n\nBy refining how you pose instructions, developers can achieve better results from existing models without retraining.\n\nFor example, if you require a blog post summary, rather than asking _\"What is this blog post about?\"_, a more appropriate response would be _\"Provide a concise, three-sentence summary and three tags for this blog post.\"_", "You could also include _\"Return the response as JSON\"_ and provide an example output to make it easier to parse in the programming language of your choice.\n\nProviding additional instructions and context in the question is known as **Zero-shot learning**.\n\n[TIP]\n.Be Positive\n====\nWhen writing a prompt, aim to provide positive instructions.\n\nFor example, when asking an expert to provide a description, you could say, \"Do not use complex words.\" However, the expert's interpretation of complex might be different from yours. Instead, say, \"Use simple words, such as ....\". This approach provides a clear instruction and a concrete example.\n====\n\n=== In-Context Learning\n\nIn-context learning provides the model with examples to inform its responses, helping it comprehend the task better.\n\nThe model can deliver more accurate answers by presenting relevant examples, especially for niche or specialized tasks.\n\nExamples could include:\n\n* Providing additional context - `When asked about \"Bats\", assume the question is about the flying mammal and not a piece of sports equipment.`  \n* Providing examples of the typical input - `Questions about capital cities will be formatted as \"What is the capital of {country}?\"`\n* Providing examples of the desired output - `When asked about the weather, return the response in the format \"The weather in {city} is {temperature} degrees Celsius.\"`\n\nProviding relevant examples for specific tasks is a form of **Few-shot learning**.\n\n=== Fine-Tuning", "Providing relevant examples for specific tasks is a form of **Few-shot learning**.\n\n=== Fine-Tuning\n\nFine-tuning involves additional language model training on a smaller, task-specific dataset after its primary training phase. This approach allows developers to specialize the model for specific domains or tasks, enhancing its accuracy and relevance. \n\nFor example, fine-tuning an existing model on your particular businesses would enhance its capability to respond to your customer's queries.\n\nThis method is the most complicated, involving technical knowledge, domain expertise, and high computational effort.\n\nA more straightforward approach would be to _ground_ the model by providing information with the prompt.\n\n=== Grounding\n\nGrounding allows a language model to reference external, up-to-date sources or databases to enrich the responses.\n\nBy integrating real-time data or APIs, developers ensure the model remains current and provides factual information beyond its last training cut-off.\n\nFor instance, if building a chatbot for a news agency, instead of solely relying on the model's last training data, grounding could allow the model to pull real-time headlines or articles from a news API. When a user asks, \"What's the latest news on the Olympics?\", the chatbot, through grounding, can provide a current headline or summary from the most recent articles, ensuring the response is timely and accurate.", "image::images/llm-news-agency.svg[A news agency chatbot, showing the user asking a question, the chatbot grounding the question with a news API, and the chatbot responding with the latest news.]\n\n== LLMs and Knowledge Graphs\n\nIn the coming lessons, you will explore these topics in detail and discover how LLMs can use Knowledge Graphs to improve their accuracy and relevance.\n\n== Check Your Understanding\n\ninclude::questions/1-temperature.adoc[leveloffset=+1]\ninclude::questions/2-external-data.adoc[leveloffset=+1]\n\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you explored the intricacies of Large Language Models (LLMs), understanding their tendencies to hallucinate and the various strategies to improve their accuracy, such as temperature settings, prompt engineering, in-context learning, fine-tuning, and grounding with external data sources like APIs.\n\nIn the next lesson, you will learn about the techniques for _grounding_ an LLM.", "= Grounding LLMs\n:order: 3\n:type: lesson\n\nIn the previous lesson, you learned about the potential methods that you could use to avoid _Hallucination_, including _Grounding_.\n\nGrounding is the process of providing context to an LLM to improve the accuracy of its responses. For developers and data scientists, grounding usually offers the highest impact for the lowest effort.\n\nYou reflected on an example chatbot for a news agency that pulled real-time headlines or articles from a news API.\n\nIn this lesson, you will explore the news agency use case in more detail.\n\n== Data Cut-Off Dates\n\nTraining a Large Language Model has a high computational cost. According to Wikipedia, OpenAI's GPT-3 model was link:https://en.wikipedia.org/wiki/GPT-3[trained on 175 billion parameters^], and the resulting trained model takes 800GB to store.\n\nRetraining a model on new data would be expensive and time-consuming. A model may take weeks or months to train.\n\nProviding real-time updates on fast-moving breaking news would be out of the question.\n\n== Retrieval Augmented Generation\n\nA news agency could train a model on static data and supplement the pre-existing knowledge base with real-time data retrieved from up-to-date sources. \n\nThis approach is known as **Retrieval Augmented Generation**, or **RAG**", "This approach is known as **Retrieval Augmented Generation**, or **RAG**\n\nRAG combines the strengths of large-scale language models with external retrieval or search mechanisms, enabling relevant information from vast datasets to be dynamically fed into the model during the generation process, thereby enhancing its ability to provide detailed and contextually accurate responses.\n\nIn summary, by adding content from additional data sources, you can improve the responses generated by an LLM.\n\n=== Benefits of RAG\n\nThe main benefit of RAG is its **enhanced accuracy**. By dynamically pulling information from external, domain-specific sources, a response grounded by RAG can provide more detailed and contextually accurate answers than a standalone LLM.\n\nRAG provides additional benefits of:\n\n* Increased transparency, as the sources of the information can be stored and examined.\n* Security, as the data sources can be secured and access controlled.\n* Accuracy and timeliness, as the data sources can be updated in real-time.\n* Access to private or proprietary data\n\n[WARNING]\n.Bad data in, bad data out\n====\nWhen prompting the LLM to respond based on the context provided, the answer will always be as good as the provided context.\n\nIf your prompt suggests pineapple as a pizza topping, don't be surprised if it suggests that you order a Hawaiian Pizza.\n====\n\nRAG could support the new agency chatbot by:", "If your prompt suggests pineapple as a pizza topping, don't be surprised if it suggests that you order a Hawaiian Pizza.\n====\n\nRAG could support the new agency chatbot by:\n\n. Accessing real-time new feeds\n. Pulling recent headlines or news articles from a database,\n. Giving this additional context to the LLM\n\nNew articles stored in a knowledge graph would be ideal for this use case. A knowledge graph could pass the LLM detail about the relationship between the entities involved and the article's metadata.\n\nFor example, when asking about the results of a recent election, the knowledge could provide additional context about the candidates, news stories relating to them, or interesting articles from the same author.\n\nimage::images/llm-news-agency-knowledge-graph.svg[A news agency chatbot using a knowledge graph to provide context to an LLM. The knowledge graph takes data from a news API and previous articles.]\n\nDuring this course, you will explore methods for implementing RAG with Neo4j, including:\n\n* Semantic Search\n* Zero-shot and few-shot learning and prompts\n* Text embedding and vector indexes with unstructured data\n* Cypher generation to gather structured data from a knowledge graph\n\n\n== Check Your Understanding\n\ninclude::questions/1-benefits.adoc[leveloffset=+1]\n\n\n[.summary]\n== Lesson Summary", "== Check Your Understanding\n\ninclude::questions/1-benefits.adoc[leveloffset=+1]\n\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you learned about the importance of grounding to enhance LLM accuracy, the computational challenges of constantly training large models, and how Retrieval Augmented Generation (RAG) combines LLMs with external data for improved responses with increased transparency.\n\nIn the next module, you will learn how to implement semantic search and use vector indexes in Neo4j.", "= Vectors & Semantic Search\n:order: 1\n:type: lesson\n\nIn the last module, you learned about the importance of grounding to improve LLM accuracy and the concept of **Retrieval Augmented Generation** (RAG).\nRAG involves providing additional information to help the LLM form a response.\n\nOne of the challenges of RAG is understanding what the user is asking for and surfacing the correct information to pass to the LLM.\n\n\n== Semantic Search vs. Traditional Keyword Search\n\nSemantic search aims to understand search phrases' intent and contextual meaning, rather than focusing on individual keywords.\n\nTraditional keyword search often depends on exact-match keywords or proximity-based algorithms that find similar words.\n\nFor example, if you input \"apple\" in a traditional search, you might predominantly get results about the fruit.\n\nHowever, in a semantic search, the engine tries to gauge the context: Are you searching about the fruit, the tech company, or something else?\n\nThe results are tailored based on the term and the perceived intent.\n\n== Vectors and embeddings\n\nIn natural language processing (NLP) and machine learning, numerical representations (known as **vectors**) represent words and phrases.\n\nEach dimension in a vector can represent a particular semantic aspect of the word or phrase. When multiple dimensions are combined, they can convey the overall meaning of the word or phrase.", "Each dimension in a vector can represent a particular semantic aspect of the word or phrase. When multiple dimensions are combined, they can convey the overall meaning of the word or phrase.\n\nA vector will not directly encode tangible attributes like color, taste, or shape.\nInstead, the model will generate a list of numerical values that closely align the word with related words such as health, nutrition, and wellness.\n\nWhen applied in a search context, the vector for \"apple\" can be compared to the vectors for other words or phrases to determine the most relevant results.\n\nYou can create vectors in various ways, but one of the most common methods is to use a **large language model**. These vectors are known as **embeddings**. With advanced models, these embeddings also contain contextual information.\n\nFor example, the embeddings for the word \"apple\" are `0.0077788467, -0.02306925, -0.007360777, -0.027743412, -0.0045747845, 0.01289164, -0.021863015, -0.008587573, 0.01892967, -0.029854324, -0.0027962727, 0.020108491, -0.004530236, 0.009129008,` ... and so on.\n\n[%collapsible]\n.Reveal the completed embeddings for the word \"apple\"!\n====\n[source]\n----\ninclude::includes/apple.txt[]\n----\n====\n\nThe vector for a word can change based on its surrounding context. For instance, the word _bank_ will have a different vector in _river bank_ than in _savings bank_.\n\nSemantic search systems can use these contextual embeddings to understand user intent.", "Semantic search systems can use these contextual embeddings to understand user intent.\n\n[NOTE]\n.Creating Vector Embeddings\n====\nLLM providers typically expose API endpoints that convert a _chunk_ of text into a vector embedding.\nDepending on the provider, the shape and size of the vector may differ.\n\nFor example, OpenAI's `text-embedding-ada-002` embedding model converts text into a vector of 1,536 dimensions.\n====\n\nYou can use the _distance_ or _angle_ between vectors to gauge the semantic similarity between words or phrases.\n\nimage::images/vector-distance.svg[A 3 dimensional chart illustrating the distance between vectors. The vectors are for the words \"apple\" and \"fruit\"]\n\nWords with similar meanings or contexts will have vectors that are close together, while unrelated words will be farther apart.\n\nThis principle is employed in semantic search to find contextually relevant results for a user's query.\n\nA semantic search involves the following steps:\n\n. The user submits a query.\n. The system creates a vector representation (embedding) of the query.\n. The system compares the query vector to the vectors of the indexed data.\n. The results are scored based on their similarity.\n. The system returns the most relevant results to the user.\n\nimage::images/semantic-vector-search.svg[A diagram show the steps of a semantic search.]", "image::images/semantic-vector-search.svg[A diagram show the steps of a semantic search.]\n\nVectors can represent more than just words. They can also represent entire documents, images, audio, or other data types. They are instrumental in the operation of many other machine-learning tasks.\n\n\n== Vectors and Neo4j\n\nVectors are the backbone of semantic search. They enable systems to understand and represent the complex, multi-dimensional nature of language, context, and meaning.\n\nSince the v5.11 release, Neo4j has a link:https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/[Vector search index^], allowing you to query for nodes based on their vector representations.\n\nIn the next lesson, you will learn how to use vectors to implement semantic search in Neo4j.\n\n[.checklist]\n\n== Check Your Understanding\n\ninclude::questions/1-semantic-vs-traditional.adoc[leveloffset=+1]\ninclude::questions/2-vector-role.adoc[leveloffset=+1]\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you learned how semantic search differs from traditional keyword search. Vectors, representing data numerically, facilitate this advanced search mechanism and are integral to many machine learning algorithms like LLMs.\n\nIn the next lesson, you will learn how to use vector indexes in Neo4j and implement semantic search.", "= Vector Indexes\n:order: 2\n:type: lesson\n:sandbox: true\n\nIn the last lesson, you learned about vectors and their role in Semantic Search.\n\nIn this lesson, you will learn how to create vector embeddings of text content in an existing Neo4j database.\n\n== Vectorizing Movie Plots\n\nGraphAcademy created a Movie Recommendation Sandbox when you enrolled in this course. The sandbox database contains over 9000 movies, 15000 actors, and over 100000 user ratings.\n\nEach movie has a `.plot` property.\n\n.Movie Plot Example\n[source,cypher]\nMATCH (m:Movie {title: \"Toy Story\"})\nRETURN m.title AS title, m.plot AS plot\n\n    \"A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy in a boy's room.\"\n\nYou can use the vector index to find the most similar movies by converting the plots into vector embeddings and comparing them.\n\nYou will use a pre-created link:https://data.neo4j.com/llm-fundamentals/openai-embeddings.csv[CSV file of 1000 movie plot vector embeddings^] in this lesson.\n\nThe CSV file contains:\n\n* `movieId` - The ID of the movie\n* `embedding` - The vector embedding of the movie plot generated by OpenAI\n\n[source,csv]\n----\nmovieId, embedding\n1, [-0.0271058, -0.0242211, 0.0060390322, -0.02437703, ...]\n2, [-0.001596838, -0.022397375, 0.0046575777, 0.0019427929, ...]\n----", "[source,csv]\n----\nmovieId, embedding\n1, [-0.0271058, -0.0242211, 0.0060390322, -0.02437703, ...]\n2, [-0.001596838, -0.022397375, 0.0046575777, 0.0019427929, ...]\n----\n\n[TIP]\n.Generating the Embeddings\n====\nlink:https://platform.openai.com/docs/guides/embeddings/what-are-embeddings[OpenAI's text-embedding-ada-002 model^] was used to create the embeddings. It is a cost-effective model that can generate embeddings for text.\n\nA simple Python script calls the embedding endpoint served by OpenAI. The link:https://github.com/neo4j-graphacademy/llm-fundamentals/blob/main/plot_openai_embeddings.py[code^] is available in the link:https://github.com/neo4j-graphacademy/llm-fundamentals[github.com/graphacademy/llm-fundamentals^] repository.\n\nEach LLM will provide an embedding in its shape. \n====\n\n== Loading Embeddings\n\nThe embeddings will be stored as a `.embedding` property on the `(:Movie)` node.\n\nYou will use the `LOAD CSV` command to load the embeddings into the Neo4j Sandbox instance.\n\nThe following Cypher loads the embeddings CSV file, performs a `MATCH` query to find the `(:Movie)` node with the corresponding `movieId` property, and then sets the `.embedding` property on that node.\n\nReview this Cypher statement before running it.", "Review this Cypher statement before running it.\n\n.Loading the Embeddings\n[source,cypher]\n----\nLOAD CSV WITH HEADERS\nFROM 'https://data.neo4j.com/llm-fundamentals/openai-embeddings.csv'\nAS row\nMATCH (m:Movie {movieId: row.movieId})\nCALL db.create.setNodeVectorProperty(m, 'embedding', apoc.convert.fromJsonList(row.embedding))\nRETURN count(*)\n----\n\nThe statement:\n\n* Loads the CSV file\n* Matches the `(:Movie)` node with the corresponding `movieId` property\n* Calls `db.create.setNodeVectorProperty()` procedure to set the `embedding` property\n* The procedure also validates that the property is a valid vector\n\nRun the statement to create the Movie embeddings.\n\nOnce complete, you can query the database to see the `.embedding` property on the `(:Movie)` nodes.\n\n[source,cypher]\nMATCH (m:Movie {title: \"Toy Story\"})\nRETURN m.title AS title, m.plot AS plot, m.embedding\n\n[TIP]\n.LOAD CSV and Strings\n====\nWhen data is loaded using `LOAD CSV`, it is treated as a string unless specifically cast using a specific function, for example, `toInteger()` or `toFloat()`.\n\nIn this case, the embedding is a string representing a JSON list, the statement coerces it into a Cypher `List` link:https://neo4j.com/docs/apoc/current/overview/apoc.convert/apoc.convert.fromJsonList/[using the `apoc.convert.fromJsonList()` procedure^].\n\nYou can link:https://graphacademy.neo4j.com/courses/importing-cypher/[learn how to use the `LOAD CSV` command in the Importing CSV Data into Neo4j course^].\n====", "You can link:https://graphacademy.neo4j.com/courses/importing-cypher/[learn how to use the `LOAD CSV` command in the Importing CSV Data into Neo4j course^].\n====\n\n== Creating the Vector Index\n\nYou will need to create a vector index to search across these embeddings.\n\nYou will use the `CREATE VECTOR INDEX` Cypher statement to create the index:\n\n.CREATE VECTOR INDEX Syntax\n[source,cypher, role=noplay nocopy]\n----\nCREATE VECTOR INDEX [index_name] [IF NOT EXISTS]\nFOR (n:LabelName)\nON (n.propertyName)\nOPTIONS \"{\" option: value[, ...] \"}\"\n----\n\n`CREATE VECTOR INDEX` expects the following parameters:\n\n* `index_name` - The name of the index\n* `LabelName` - The node label on which to index\n* `propertyName` - The property on which to index\n* `OPTIONS` - The options for the index, where you can specify:\n** `vector.dimensions` - The dimension of the embedding e.g. OpenAI embeddings consist of `1536` dimensions.\n** `vector.similarity_function` - The similarity function to use when comparing values in this index - this can be `euclidean` or `cosine`.\n\nReview and run the following Cypher to create the vector index:\n\n.Create the vector index\n[source,cypher]\n----\nCREATE VECTOR INDEX moviePlots IF NOT EXISTS\nFOR (m:Movie)\nON m.embedding\nOPTIONS {indexConfig: {\n `vector.dimensions`: 1536,\n `vector.similarity_function`: 'cosine'\n}}\n----", "Note that the index is called `moviePlots`, it is against the `Movie` label, and it is on the `.embedding` property. The `vector.dimensions` is `1536` (as used by OpenAI) and the `vector.similarity_function` is `cosine`. The `IF NOT EXISTS` clause ensures that the statement only creates the index if it does not already exist.\n\nRun the statement to create the index.\n\n[TIP]\n.Choosing a Similarity Function\n====\nGenerally, cosine will perform best for text embeddings, but you may want to experiment with other functions.\n\nYou can link:https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/#indexes-vector-similarity[read more about similarity functions in the documentation^].\n\nTypically, you will choose a similarity function closest to the loss function used when training the embedding model. You should refer to the model's documentation for more information.\n====\n\n=== Check the index creation status\n\nThe index will be updated asynchronously. You can check the status of the index population using the `SHOW INDEXES` statement:\n\nCheck that you created the index successfully using the `SHOW INDEXES` command.\n\n.Show Indexes\n[source,cypher]\n----\nSHOW INDEXES  YIELD id, name, type, state, populationPercent WHERE type = \"VECTOR\"\n----\n\nYou should see a result similar to the following:\n\n.Show Indexes Result\n|===\n| id | name | type | state | populationPercent\n\n|1 | \"moviePlots\" | \"VECTOR\" | \"ONLINE\" | `100.0`\n|===", "You should see a result similar to the following:\n\n.Show Indexes Result\n|===\n| id | name | type | state | populationPercent\n\n|1 | \"moviePlots\" | \"VECTOR\" | \"ONLINE\" | `100.0`\n|===\n\nOnce the `state` is listed as online, the index will be ready to query.\n\nThe `populationPercentage` field indicates the proportion of node and property pairing.\n\nWhen the `populationPercentage` is `100.0`, all the movie embeddings have been indexed.\n\n== Querying Vector Indexes\n\nYou can query the index using the `db.index.vector.queryNodes()` procedure.\n\nThe procedure returns the requested number of approximate nearest neighbor nodes and their similarity score, ordered by the score.\n\n.db.index.vector.queryNodes Syntax\n[source,cypher,role=nocopy noplay]\n----\nCALL db.index.vector.queryNodes(\n    indexName :: STRING,\n    numberOfNearestNeighbours :: INTEGER,\n    query :: LIST<FLOAT>\n) YIELD node, score\n----\n\nThe procedure accepts three parameters:\n\n1. `indexName` - The name of the vector index\n2. `numberOfNearestNeighbours` - The number of results to return\n3. `query` - A list of floats that represent an embedding\n\nThe procedure yields two arguments: \n\n. A `node` which matches the query\n. A similarity `score` ranging from `0.0` to `1.0`.\n\n// TODO - MH comment - should we also show how to search for a user question embedding? should we prepare a few in a file?\n\nYou can use this procedure to find the closest embedding value to a given embedding.\n\nFor example, find movies with a similar plot to another.", "You can use this procedure to find the closest embedding value to a given embedding.\n\nFor example, find movies with a similar plot to another.\n\nReview this Cypher before running it.\n\n[source,cypher]\n.Similar Plots\n----\nMATCH (m:Movie {title: 'Toy Story'})\n\nCALL db.index.vector.queryNodes('moviePlots', 6, m.embedding)\nYIELD node, score\n\nRETURN node.title AS title, node.plot AS plot, score\n----\n\nThe query finds the _Toy Story_ `Movie` node and uses the `.embedding` property to find the most similar plots. The `db.index.vector.queryNodes()` procedure uses the `moviePlots` vector index to find similar embeddings.\n\nRun the query. The procedure returns the requested number of approximate nearest neighbor nodes and their similarity score, ordered by the score.", "Run the query. The procedure returns the requested number of approximate nearest neighbor nodes and their similarity score, ordered by the score.\n\n.Similar Plots Results\n|===\n| title | plot | score\n| \"Toy Story\" | \"A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy in a boy's room.\" | 1.0\n| \"Little Rascals, The\" | \"Alfalfa is wooing Darla and his He-Man-Woman-Hating friends attempt to sabotage the relationship.\" | 0.9214372634887695\n| \"NeverEnding Story III, The\" | \"A young boy must restore order when a group of bullies steal the magical book that acts as a portal between Earth and the imaginary world of Fantasia.\" | 0.9206198453903198\n|  \"Drop Dead Fred\" | \"A young woman finds her already unstable life rocked by the presence of a rambunctious imaginary friend from childhood.\" | 0.9199690818786621\n| \"E.T. the Extra-Terrestrial\" | \"A troubled child summons the courage to help a friendly alien escape Earth and return to his home-world.\" | 0.919100284576416\n| \"Gumby: The Movie\" | \"In this offshoot of the 1950s claymation cartoon series, the crazy Blockheads threaten to ruin Gumby's benefit concert by replacing the entire city of Clokeytown with robots.\" | 0.9180967211723328\n|===\n\nThe similarity score is between `0.0` and `1.0`, with `1.0` being the most similar. Note how the most similar plot is that of the _Toy Story_ movie itself!\n\n== Considerations", "The similarity score is between `0.0` and `1.0`, with `1.0` being the most similar. Note how the most similar plot is that of the _Toy Story_ movie itself!\n\n== Considerations\n\nAs you can see, this approach is relatively straightforward and can quickly yield results. The downside to this approach is that it relies heavily on the embeddings and similarity function to produce valid results.\n\nThis approach is also a black box - with 1536 dimensions, and it would be impossible to determine how the vectors are structured and how they influenced the similarity score.\n\nThe movies returned look similar, but without reading and comparing them, you would have no way of verifying that the results are correct.\n\n== Check your understanding\n\ninclude::questions/1-create-index.adoc[leveloffset=+1]\ninclude::questions/2-query-index.adoc[leveloffset=+1]\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you learned how to create, populate, and use a Vector index in Neo4j.\n\nIn the next lesson, you will learn how to use feedback to improve the suggestions provided by Semantic Search.", "= Improving Semantic Search\n:order: 3\n:type: lesson\n:optional: true\n\nIn this module, you have learned about the benefits and drawbacks of Semantic Search using an underlying vector index.\n\nYou received an answer when querying the vector index for a similar movie, but it is difficult to tell whether that is the best movie in the database.\n\nWhen using vector-backed semantic search, you are placing a lot of emphasis on the underlying model to provide a good similarity.\n\nThe context from which the similarity scores are provided is an important factor here.\n\nThe embeddings were trained on a generic dataset, and as such, may identify two movies with a main character called Jack who likes fruit as movies with a close similarity.\n\nThis is correct in one sense; they are both movies and very similar when compared to a cricket bat or a vanilla yoghurt.\n\nOn the other hand, if you are recommending a movie to pass away a rainy afternoon, a fairy tale for children is very different to a horror film.\n\n== Incorporating Graph Features\n\nGraph features can enrich vector-backed semantic search by providing structural and relational context to data.\n\nBy leveraging the connections in graphs, semantic search can draw upon relationships and hierarchies between entities, enhancing the depth and relevance of search results.", "By leveraging the connections in graphs, semantic search can draw upon relationships and hierarchies between entities, enhancing the depth and relevance of search results.\n\nWhile vectors capture semantic nuances, adding graph structures allows for a more holistic understanding of data, considering its inherent meaning and position in a broader knowledge network.\n\n//TODO - Do we want to embed this video? I think it will age quickly\n\n// This topic is out of the scope of this module, but for more information, watch link:https://www.youtube.com/watch?v=bRD09ndyJNs[Going Meta - Ep 21: Vector-based Semantic Search and Graph-based Semantic Search^], in which Dr Jesus Barrasa and Alexander Erdl explore the differences between Vector-based Semantic Search and Graph-based Semantic Search.\n\n// image::images/jesus-barrassa.png[Dr Jesus Barrasa]\n// _Dr Jesus Barrasa_\n\n== Using Feedback\n\nA more straightforward approach may be to use user feedback to gradually re-rank and curate the content returned by a vector index search.\n\nGraphAcademy has a chatbot that uses this approach to improve the quality of its responses.\n\n[NOTE]\n.The GraphAcademy Chatbot\n====\nYou can access the GraphAcademy chatbot by clicking on the chat icon image:images/chat-icon.png[chat icon] in the bottom right corner of the GraphAcademy website.\n====\n\nWhen a user asks a question, the server generates an embedding of the question and uses the vector index to identify relevant content.", "When a user asks a question, the server generates an embedding of the question and uses the vector index to identify relevant content.\n\nThis approach works well in most cases, but now and again, the index fails to return relevant content, in which case the server instructs the LLM to respond to the user with an apology.\n\nGraphAcademy stores questions, responses, and the user's feedback in the database. Over time, a better picture of which documents do or do not answer specific queries emerges. \n\nThe chatbot can use this context to improve future responses. For example, when a user asks a question, the chatbot can compare it to previous questions and exclude any suggested documents where the answer was previously unhelpful.\n\n[TIP]\n.Want to know more?\n====\nThe article link:https://medium.com/neo4j/building-an-educational-chatbot-for-graphacademy-with-neo4j-f707c4ce311b[Building an Educational Chatbot for GraphAcademy with Neo4j Using LLMs and Vector Search^] describes in more detail how we ingested Neo4j Documentation and GraphAcademy lessons, using embeddings of the content to populate a vector index.\n\nimage::images/chatbot-data-model.png[The GraphAcademy Chatbot Data Model]\n====\n\n\nread::Continue[]\n\n[.summary]\n\n== Module Summary\n\nIn this module, you have learned how to implement vector search in Neo4j.", "read::Continue[]\n\n[.summary]\n\n== Module Summary\n\nIn this module, you have learned how to implement vector search in Neo4j.\n\nYou have learned how to create a vector index using the `db.index.vector.createNodeIndex()` procedure, set vector properties using the `db.create.setVectorProperty()` procedure, and query the vector index using the `db.index.vector.queryNodes()` procedure.\n\nYou also explored the benefits and potential drawbacks of Vector-based Semantic Search.\n\nIn the next module, you will get hands-on with Langchain, a framework designed to simplify the creation of applications using large language models.", "= An introduction to Langchain\n:order: 1\n:type: lesson\n\nThis lesson will teach you about Langchain, an open-source framework for building AI applications.\n\n== What is Langchain?\n\nlink:https://langchain.com[LangChain^] is designed to accelerate the development of LLM applications.\n\nLangchain enables software developers to build LLM applications quickly, integrating with external components and data sources.\n\nDevelopers can build Langchain applications with link:https://python.langchain.com/[Python^] or link:https://js.langchain.com/[JavaScript/TypeScript^].\n\nLangchain supports multiple LLMs and allows you to swap one for another with a single parameter change.\n\nMeaning you can quickly test multiple LLMs for suitability and utilize different models for different use cases.\n\nLangchain provides out-of-the-box integrations with APIs and databases including link:https://python.langchain.com/docs/integrations/providers/neo4j[Neo4j^].\n\nLangchain's flexibility allows you to test different LLM providers and models with minimal code changes.\n\n== How does it work?\n\nLangChain applications bridge users and LLMs, communicating back and forth with the LLM through **Chains**.\n\nThe key components of a Langchain application are:\n\n* **Model Interaction (Model I/O)**: Components that manage the interaction with the language model, overseeing tasks like feeding inputs and extracting outputs.", "* **Model Interaction (Model I/O)**: Components that manage the interaction with the language model, overseeing tasks like feeding inputs and extracting outputs.\n\n* **Data Connection and Retrieval:** Retrieval components can access, transform, and store data, allowing for efficient queries and retrieval.\n\n* **Chains:** Chains are reusable components that determine the best way to fulfill an instruction based on a _prompt_.\n\n* **Agents:** Agents orchestrate commands directed at LLMs and other tools, enabling them to perform specific tasks or solve designated problems.\n\n* **Memory:** Allow applications to retain context, for example, remembering the previous messages in a conversation.\n\n[%collapsible]\n.Click to reveal an example Langchain application\n====\nThis example program uses Langchain to build a chatbot that answers questions about Neo4j Cypher queries.\n\nThe program interacts with an OpenAI LLM, uses a prompt template to instruct the LLM on _how to act_, and uses a memory component to retain context.\n\nAfter completing this module, you will understand what this program does and how it works.\n\n[source, python]\n----\ninclude::code/example_applcation.py[][]\n----\n====\n\nIn the next lesson, you will set up your development environment and use Langchain to query an LLM.\n\n== Check Your Understanding\n\ninclude::questions/1-languages.adoc[leveloffset=+1]\n\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you learned about Langchain, an open-source framework for building AI applications.", "include::questions/1-languages.adoc[leveloffset=+1]\n\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you learned about Langchain, an open-source framework for building AI applications.\n\nIn the next lesson, you will learn how to set up your development environment and use Langchain to query an LLM.", "= Initialising the LLM\n:order: 2\n:type: lesson\n\nIn this lesson, you will:\n\n* Install the Langchain Python package\n* Initialize an LLM\n* Get a response from the LLM using a *Prompt*\n\n[IMPORTANT]\n.Python v3.12+\nAs of writing, link:https://github.com/langchain-ai/langchain/issues/11479[Langchain doesn't support Python v3.12 or above^]. You can download Python v3.11 from link:https://www.python.org/downloads/[python.org/downloads^].\n\n== Choosing your LLM\n\nLangchain supports multiple LLM providers (link:https://openai.com/[OpenAI^], link:https://cohere.com/[Cohere^], link:https://ollama.ai/[Ollama^] and more). The quality and cost vary. LLMs are also trained for different purposes, so it is worth experimenting with models and prompts. Depending on your use case, different LLMs may return better results.\n\nThis course includes instructions for using link:https://openai.com/[OpenAI^], but the same principles apply to all LLMs.\n\n[NOTE]\n.Using OpenAI\nIf you wish to use OpenAI and follow this course's practical activities, you must create an account and set up billing.\n\n== Setup\n\n=== Installing Langchain\n\nTo use Langchain, you must install the `langchain` package and its dependencies using `pip`:\n\n[source,sh]\n.Install Langchain\npip install langchain", "== Setup\n\n=== Installing Langchain\n\nTo use Langchain, you must install the `langchain` package and its dependencies using `pip`:\n\n[source,sh]\n.Install Langchain\npip install langchain\n\n[TIP]\n.Virtual Environment\n====\nYou may find it helpful to create a Python virtual environment using a tool like link:https://virtualenv.pypa.io/en/latest/[virtualenv^]. Using a virtual environment allows you to install packages without affecting your system Python installation.\n====\n\n=== Installing OpenAI\n\nYou will need to setup OpenAI at link:https://platform.openai.com[platform.openai.com], including:\n\n* Creating an account\n* Creating an API key\n* Setting up billing\n\nYou can install the `openai` and `langchain-openai` Python packages using `pip`:\n\n[source,sh]\n.Install OpenAI SDK\npip install openai langchain-openai\n\n== Create a Langchain application\n\nCreate a new Python program and copy this code into it.\n\n[source,python]\n----\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(openai_api_key=\"sk-...\")\n\nresponse = llm.invoke(\"What is Neo4j?\")\n\nprint(response)\n----\n\n[IMPORTANT]\n.OpenAI API Key\nRemember to include your OpenAI API key in the `openai_api_key` parameter.\n\nReview the program and note the following:\n\n* That `OpenAI` is used as the LLM\n* You can pass a question to the `llm.invoke` and get a response\n\nRunning the program should produce a response from the LLM similar to:", "* That `OpenAI` is used as the LLM\n* You can pass a question to the `llm.invoke` and get a response\n\nRunning the program should produce a response from the LLM similar to:\n\n    Neo4j is an open-source, NoSQL graph database management system developed by Neo4j, Inc. It is designed to store and manage data in a graph-like structure using nodes, relationships, and properties. Neo4j is used for a wide variety of use cases including real-time data analytics, fraud detection, recommendation engines, and network and IT operations.\n\nTry modifying the program to ask a different question.\n\n== Prompts\n\nPrompt templates allow you to create reusable instructions or questions. You can use them to create more complex or structured input for the LLM.\n\nBelow is an example of a prompt template:\n\n[source, python]\n----\n\"\"\"\nYou are a cockney fruit and vegetable seller.\nYour role is to assist your customer with their fruit and vegetable needs.\nRespond using cockney rhyming slang.\n\nTell me about the following fruit: {fruit}\n\"\"\"\n----\n\nThis prompt template would give context to the LLM and instruct it to respond as a cockney fruit and vegetable seller.\n\nYou can define parameters within the template using braces `{}` e.g. `{fruit}`. These parameters will be replaced with values when the prompt is formatted.\n\nModify your program to use the prompt template:\n\n[source,python]\n----\nfrom langchain.prompts import PromptTemplate", "Modify your program to use the prompt template:\n\n[source,python]\n----\nfrom langchain.prompts import PromptTemplate\n\ntemplate = PromptTemplate(template=\"\"\"\nYou are a cockney fruit and vegetable seller.\nYour role is to assist your customer with their fruit and vegetable needs.\nRespond using cockney rhyming slang.\n\nTell me about the following fruit: {fruit}\n\"\"\", input_variables=[\"fruit\"])\n----\n\nCall the LLM, passing the formatted prompt template as the input:\n\n[source,python]\n----\nresponse = llm.invoke(template.format(fruit=\"apple\"))\n\nprint(response)\n----\n\nYou use the `format` method to pass the parameters to the prompt e.g. `fruit=\"apple\"`. The input variables will be validated when the prompt is formatted, and a `KeyError` will be raised if any variables are missing from the input.\n\nThe prompt will be formatted as follows:\n\n    You are a cockney fruit and vegetable seller.\n    Your role is to assist your customer with their fruit and vegetable needs\n    Respond using cockney rhyming slang.\n    Tell me about the following fruit: apple\n\nWhen running the program, you should see a response similar to:\n\n    Well, apples is a right corker - they come in all shapes and sizes from Granny Smiths to Royal Galas. Got 'em right 'ere, two a penny - come and grab a pick of the barrel!\n\n[NOTE]\n.Differing Results\nIf you run the program multiple times, you will notice you get different responses because the LLM is generating the answer each time.", "[NOTE]\n.Differing Results\nIf you run the program multiple times, you will notice you get different responses because the LLM is generating the answer each time.\n\nBefore moving on to the next lesson, try creating a prompt template and using it to get a response from the LLM.\n\n[TIP]\n.Creating PromptTemplates\nYou can create a prompt from a string by calling the `PromptTemplate.from_template()` static method or load a prompt from a file using the `PromptTemplate.from_file()` static method.\n\n== Configuring the LLM\n\nWhen you create the LLM, you can configure it with parameters such as the `temperature` and `model`.\n\n[source,python]\n----\nllm = OpenAI(\n    openai_api_key=\"sk-...\",\n    model=\"gpt-3.5-turbo-instruct\",\n    temperature=0\n)\n----\n\nWhen selecting a model, it is worth considering the quality of the output and the cost per token. There are several link:https://platform.openai.com/docs/models/overview[OpenAI models^] available, each with different characteristics.\n\nAs you learned in the first module, all prompts have a `temperature`. The temperature, between `0.0` and `1.0`, affects the randomness or creativeness of the response.\n\nNote how the code above uses the \"gpt-3.5-turbo-instruct\" model with a temperature of `0.0`. Typically, this will produce a good response, as grounded in fact as possible.\n\n== Check Your Understanding\n\ninclude::questions/1-prompts.adoc[leveloffset=+1]\n\n[.summary]\n== Lesson Summary", "== Check Your Understanding\n\ninclude::questions/1-prompts.adoc[leveloffset=+1]\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you learned how to communicate with an LLM with text and prompt templates.\n\nIn the next lesson, you will learn all about Langchain **Chains**.", "= Chains\n:order: 3\n:type: lesson\n\nIn this lesson, you will learn about chains and how to use them to create reusable components.\n\nChains allows you to combine language models with different data sources and third-party APIs.\n\n== Using LLMChain\n\nThe simplest chain is an `LLMChain`. An `LLMChain` combines a prompt template with an LLM and returns a response.\n\nPreviously, you created a program that used a prompt template and an LLM to generate a response about fruit.\n\n[%collapsible]\n.Click to reveal the code for the program.\n====\n[source,python]\n----\ninclude::../2-initialising-the-llm/code/llm_prompt.py[]\n----\n====\n\nYou can combine this program into a chain and create a reusable component.\n\n[source,python]\n----\ninclude::code/llm_chain.py[]\n----\n\nNote how the `llm_chain.invoke` method accepts the template parameters as a dictionary.\n\n[source,python]\n----\nresponse = llm_chain.invoke({\"fruit\": \"apple\"})\n----\n\nThe `response` is a dictionary containing the template parameters and a `text` key containing the response from the LLM.\n\n    {'fruit': 'apple', 'text': 'Well, mate, apples and pears are what I sell...'}\n\n== Output Parsers\n\nYou can specify that a chain uses a specific link:https://python.langchain.com/docs/modules/model_io/output_parsers/[output parser^] to parse the output of the LLM.\n\nSetting the `output_parser` parameter to `StrOutputParser` on the `LLMChain` would ensure the response is a string.\n\n[source,python]\n----\nfrom langchain.schema import StrOutputParser", "Setting the `output_parser` parameter to `StrOutputParser` on the `LLMChain` would ensure the response is a string.\n\n[source,python]\n----\nfrom langchain.schema import StrOutputParser\n\nllm_chain = LLMChain(\n    llm=llm,\n    prompt=template,\n    output_parser=StrOutputParser()\n)\n----\n\nYou can change the prompt to instruct the LLM to return a specific output type. For example, return JSON by specifying `Output JSON` and give a format in the prompt:\n\n[source,python]\n----\ntemplate = PromptTemplate.from_template(\"\"\"\nYou are a cockney fruit and vegetable seller.\nYour role is to assist your customer with their fruit and vegetable needs.\nRespond using cockney rhyming slang.\n\nOutput JSON as {{\"description\": \"your response here\"}}\n\nTell me about the following fruit: {fruit}\n\"\"\")\n----\n\nYou can ensure Langchain parses the response as JSON by specifying `SimpleJsonOutputParser` as the `output_parser`:\n\n[source,python]\n----\nfrom langchain.output_parsers.json import SimpleJsonOutputParser\n\nllm_chain = LLMChain(\n    llm=llm,\n    prompt=template,\n    output_parser=SimpleJsonOutputParser()\n)\n----\n\nThe benefits of using chains are:\n\n* **Modularity**: LangChain provides many modules that can be used to build language model applications. These modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.", "* **Customizability**: Most LangChain applications allow you to configure the LLM and/or the prompt used, so knowing how to take advantage of this will be a big enabler.\n\n* **Ease** of Use: The components are designed to be easy to use, regardless of whether you are using the rest of the LangChain framework or not.\n\n* **Standard** Interface: LangChain provides a standard interface for chains, enabling developers to create sequences of calls that go beyond a single LLM call.\n\n== Check Your Understanding\n\ninclude::questions/1-chains.adoc[leveloffset=+1]\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you learned about LLM chains and how they can group a prompt, LLM, and output parser.\n\nIn the next lesson, you will learn about chat models.", "= Chat Models\n:order: 4\n:type: lesson\n\nIn the previous lesson, you learned how to communicate with the OpenAI LLM using Langchain.\n\nThe communication was simple, you provided a prompt and got a response.\n\nIn this lesson, you will learn how to use a chat model to _have a conversation_ with the LLM.\n\n== Chat Models vs Language Models\n\nUntil now, you have been using a language model to communicate with the LLM. A language model predicts the next word in a sequence of words. Chat models are designed to have conversations - they accept a list of messages and return a conversational response.\n\nChat models typically support different types of messages:\n\n* System - System messages instruct the LLM on how to act on human messages\n* Human - Human messages are messages sent from the user\n* AI - Responses from the AI are called AI Responses\n\n== Create a Chat Model\n\nYou are going to create an application that uses a chat model.\n\nThe application will:\n\n* Use a system message to provide instructions\n* Use a human message to ask a question\n* Receive an AI response to the question\n\nCreate a new Python program, import the required LangChain modules and instantiate the chat model.\n\n[IMPORTANT]\n.OpenAI API Key\nRemember to update the API key with your own.\n\n[source,python]\n----\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, SystemMessage  \n\nchat_llm = ChatOpenAI(\n    openai_api_key=\"sk-...\"\n)\n----", "[source,python]\n----\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, SystemMessage  \n\nchat_llm = ChatOpenAI(\n    openai_api_key=\"sk-...\"\n)\n----\n\nCreate a system message to provide instructions to the chat model using the link:https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html[`SystemMessage`^] class.\n\n[source,python]\n----\ninstructions = SystemMessage(content=\"\"\"\nYou are a surfer dude, having a conversation about the surf conditions on the beach.\nRespond using surfer slang.\n\"\"\")\n----\n\nCreate a link:https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html[`HumanMessage`^] object to ask a question.\n\n[source,python]\nquestion = HumanMessage(content=\"What is the weather like?\")\n\nYou can now call the chat model passing a list of messages. In this example, pass the system message with the instructions and the human message with the question.\n\n[source,python]\n----\nresponse = chat_llm.invoke([\n    instructions,\n    question\n])\n\nprint(response.content)\n----\n\n[%collapsible]\n.Reveal the complete code\n====\n[source,python]\n----\ninclude::code/chat-model.py[]\n----\n====\n\nThe `response` is an link:https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html[`AIMessage`^] object.", "The `response` is an link:https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html[`AIMessage`^] object.\n\n[source,python]\nAIMessage(content=\"Dude, the weather is totally gnarly! It's sunny with some epic offshore winds. Perfect conditions for shredding some sick waves!\", additional_kwargs={}, example=False)\n\n=== Wrapping in a Chain\n\nYou can create a reusable chat model chain using what you have learned about prompts and chains.\n\nRather than passing a list of messages to the chat model, you can create a prompt that gives context to the conversation and then pass the question to the chat model.\n\nReview this program and identify the following:\n\n* The prompt provides the instructions to the LLM.\n* The chain is created using the chat model and the prompt.\n* The question is passed to the chat model as a parameter of the `invoke` method.\n\n[source,python]\n----\ninclude::code/chat-model-chain.py[]\n----\n\nCreating a chain is the first step to creating a more sophisticated chat model. You can use chains to combine different elements into one call and support more complex features.\n\n=== Giving context\n\nPreviously, you learned about grounding and how it can provide context to the LLM and avoid _Hallucination_.\n\nCurrently the chat model is not grounded; it is unaware of surf conditions on the beach. It responds based on the question and the LLMs training data (which could be months or years out of date).", "Currently the chat model is not grounded; it is unaware of surf conditions on the beach. It responds based on the question and the LLMs training data (which could be months or years out of date).\n\nYou can ground the chat model by providing information about the surf conditions on the beach.\n\nReview this example where the chat model can access current beach conditions (`current_weather`) as a variable (`context`) in the prompt.\n\n[source,python]\n----\ninclude::code/chat-model-context.py[]\n----\n\nRun the program and predict what the response will be.\n\n[%collapsible]\n.Click to reveal the response\n====\nBelow is a typical response. The LLM has used the context passed in the prompt to provide a more accurate response.\n\n    {\n        'context': {\n            \"surf\": [\n                {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n                {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n                {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n            ]},\n        'question': 'What is the weather like on Watergate Bay?',\n        'text': \"Dude, the surf at Watergate Bay is pumping! We got some sick 3ft waves rolling in, but unfortunately, we got some onshore winds messing with the lineup. But hey, it's all good, still plenty of stoke to be had out there!\"\n    }\n====\n\nInvestigate what happens when you change the context by adding additional beach conditions.", "Investigate what happens when you change the context by adding additional beach conditions.\n\nProviding context is one aspect of Retrieval Augmented Generation (RAG). In this program, you _manually_ gave the model context; however, you could have retrieved real-time information from an API or database.\n\n== Check Your Understanding\n\ninclude::questions/1-message-types.adoc[leveloffset=+1]\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you learned that chat models are designed for conversations and how to add additional context.\n\nIn the next lesson, you will learn how to give your chat model a memory so it can retain information between questions.", "= Giving a Chat Model Memory\n:order: 5\n:type: lesson\n\nFor a chat model to be helpful, it needs to remember what messages have been sent and received.\n\nWithout the ability to _remember_ the chat model will not be able to act according to the context of the conversation.\n\nFor example, without a memory the conversation may go in circles:\n\n    [user] Hi, my name is Martin\n\n    [chat model] Hi, nice to meet you Martin\n\n    [user] Do you have a name?\n\n    [chat model] I am the chat model. Nice to meet you. What is your name?\n\n== Conversation Memory\n\nIn the previous lesson, you created a chat model that responds to user's questions about surf conditions.\n\n[%collapsible]\n.Reveal the code\n====\n[source,python]\n----\ninclude::../3-chat-models/code/chat-model-context.py[]\n----\n====\n\nIn this lesson, you will add a memory to this program.\n\nLangChain supports several link:https://python.langchain.com/docs/modules/memory/[memory types^], which support different scenarios.\n\nYou will use the link:https://python.langchain.com/docs/modules/memory/types/buffer[Conversation Buffer] memory type to store the conversation history between you and the LLM.\n\nAs each call to the LLM is stateless, you need to include the chat history in every call to the LLM. You can modify the prompt template to include the chat history in a variable.\n\n[source, python]\n----\nprompt = PromptTemplate(template=\"\"\"You are a surfer dude, having a conversation about the surf conditions on the beach.\nRespond using surfer slang.", "[source, python]\n----\nprompt = PromptTemplate(template=\"\"\"You are a surfer dude, having a conversation about the surf conditions on the beach.\nRespond using surfer slang.\n\nChat History: {chat_history}\nContext: {context}\nQuestion: {question}\n\"\"\", input_variables=[\"chat_history\", \"context\", \"question\"])\n----\n\nThe `chat_history` variable will contain the conversation history.\n\nYou can now create the `ConversationBufferMemory` and pass it to the `LLMChain`:\n\n[source, python]\n----\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n\nchat_chain = LLMChain(llm=chat_llm, prompt=prompt, memory=memory)\n----\n\nThere are three parameters to the `ConversationBufferMemory`:\n\n* `memory_key` - is the variable in the prompt that will hold the conversation history\n* `input_key` - is the variable in the prompt that will hold the user's question to the chat model\n* `return_messages` - when True, the conversation history will be returned as a list\n\nWhen you create the `LLMChain`, the `memory` parameter should be set to the `ConversationBufferMemory` instance.\n\nWhen you ask the chat model multiple questions, the LLM will use the context from the previous questions when responding.\n\n[source, python]\n----\nresponse = chat_chain.invoke({\n    \"context\": current_weather,\n    \"question\": \"Hi, I am at Watergate Bay. What is the surf like?\"\n})\nprint(response[\"text\"])", "[source, python]\n----\nresponse = chat_chain.invoke({\n    \"context\": current_weather,\n    \"question\": \"Hi, I am at Watergate Bay. What is the surf like?\"\n})\nprint(response[\"text\"])\n\nresponse = chat_chain.invoke({\n    \"context\": current_weather,\n    \"question\": \"Where I am?\"\n})\nprint(response[\"text\"])\n----\n\n    [user] Hi, I am at Watergate Bay. What is the surf like?\n\n    [chat model] Dude, stoked you're at Watergate Bay! The surf is lookin' pretty chill, about 3ft waves rollin' in. But watch out for those onshore winds, they might mess with your flow.\n\n    [user] Where I am?\n\n    [chat model] You're at Watergate Bay, dude!\n\n[TIP]\n.See the conversation history\n====\nYou can set the `LLMChain` `verbose` parameter to `True` to see the conversation history in the console.\n[source, python]\n----\nchat_chain = LLMChain(llm=chat_llm, prompt=prompt, memory=memory, verbose=True)\n----\n====\n\nTry creating a simple loop and ask the chat model a few questions:\n\n[source, python]\n----\nwhile True:\n    question = input(\"> \")\n    response = chat_chain.invoke({\n        \"context\": current_weather,\n        \"question\": question\n        })\n\n    print(response[\"text\"])\n----\n\n[%collapsible]\n.Click to reveal the complete code.\n====\n[source,python]\n----\ninclude::code/chat-model-memory.py[]\n----\n====\n\n== Check Your Understanding\n\ninclude::questions/1-state.adoc[leveloffset=+1]\n\n[.summary]\n== Lesson Summary", "== Check Your Understanding\n\ninclude::questions/1-state.adoc[leveloffset=+1]\n\n[.summary]\n== Lesson Summary\n\nIn this lesson, you learned how to use the `ConversationBufferMemory` to store the conversation history between you and the LLM.\n\nIn the next lesson, you will learn how to create an agent to give an LLM access to different tools and data sources.", "= Agents\n:order: 6\n:type: lesson\n\nIn this lesson, you will learn how to create an link:https://python.langchain.com/docs/modules/agents[agent^].\n\nAgents wrap a model and give it access to a set of _tools_. These tools may access additional data sources, APIs, or functionality. The model is used to determine which of the tools to use to complete a task.\n\nThe agent you will create will be able to chat about movies and search YouTube for movie trailers.\n\n== Tools\n\nA tool is a specific abstraction around a function that makes it easy for a language model to interact with it. link:https://python.langchain.com/docs/integrations/tools[Langchain provides several tools^] out of the box, and you can create tools to extend the functionality of your agents.\n\nTools can be grouped into link:https://python.langchain.com/docs/integrations/toolkits/[toolkits^] to solve a particular problem. For example, a toolkit that gets YouTube information - videos, thumbnails, transcripts, views, etc.\n\nYou will use the link:https://python.langchain.com/docs/integrations/tools/youtube/[YouTubeSearchTool^] to search YouTube for movie trailers.\n\n== Movie trailer agent\n\nReview the program below, before running it.\n\n[source,python]\n----\ninclude::code/chat-agent.py[]\n----\n\n[%collapsible]\n.Click here to see a typical output from this program\n====\n    [user] Find a movie where aliens land on earth.", "[source,python]\n----\ninclude::code/chat-agent.py[]\n----\n\n[%collapsible]\n.Click here to see a typical output from this program\n====\n    [user] Find a movie where aliens land on earth.\n\n    [chat model] Sure, I can help you with that. One movie I would recommend where aliens land on Earth is \"Arrival\" (2016). It's a science fiction film directed by Denis Villeneuve. The story follows a linguist who is recruited by the military to help communicate with an alien species that has landed on Earth. It's a thought-provoking and visually stunning movie that explores themes of communication, time, and the human experience. I hope you enjoy it!\n====\n\nBased on your understanding from previous lessons, you should be able to identify the following:\n\n. A chat model is being used to have a conversation about movies\n. The prompt which sets the context for the LLM and the input variables\n. That memory is used to store the conversation history\n. A chain is created to link the chat model, prompt, and memory together\n\nIn addition to the above, the following is new:", "In addition to the above, the following is new:\n\n. A tool is created using the chain:\n+\n[source,python]\n----\ntools = [\n    Tool.from_function(\n        name=\"Movie Chat\",\n        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n        func=chat_chain.run,\n        return_direct=True\n    )\n]\n----\n. An agent is created that uses the tool:\n+\n[source, python]\n----\nagent_prompt = hub.pull(\"hwchase17/react-chat\")\nagent = create_react_agent(llm, tools, agent_prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)\n----\n\n### Creating tools\n\nTools are interfaces that an agent can interact with. You can link:https://python.langchain.com/docs/modules/agents/tools/custom_tools[create custom tools^] able to perform any functionality you want.\n\nIn this example, the Tool is created from a function. The function is the `chat_chain.run` method.\n\n[source, python]\n----\ntools = [\n    Tool.from_function(\n        name=\"Movie Chat\",\n        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n        func=chat_chain.run,\n        return_direct=True\n    )\n]\n----\n\nThe `name` and `description` help the LLM select which tool to use when presented with a question. The `func` parameter is the function that will be called when the tool is selected. The `return_direct` flag indicates that the tool will return the result directly.", "Agents support multiple tools, so you pass them to the agent as a list (`tools`).\n\n### Initializing an agent\n\nThe following code creates the agent:\n\n[source, python]\n----\nagent_prompt = hub.pull(\"hwchase17/react-chat\")\nagent = create_react_agent(llm, tools, agent_prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)\n----\n\nThere are different link:https://python.langchain.com/docs/modules/agents/agent_types/[types of agents^] that you can create. This example creates a _ReAct_ - Reasoning and Acting) agent type.\n\nAn agent requires a prompt. You could create a prompt, but in this example, the program pulls a pre-existing prompt from the link:https://smith.langchain.com/hub/[Langsmith Hub^].\n\nThe link:https://smith.langchain.com/hub/hwchase17/react-chat?organizationId=d9a804f5-9c91-5073-8980-3d7112f1cbd3[`hwcase17/react-chat`^] prompt instructs the model to provide an answer using the tools available in a specific format.\n\nThe `create_react_agent` function creates the agent and expects the following parameters:\n\n* The `llm` that will manage the interactions and decide which tool to use\n* The `tools` that the agent can use\n* The `prompt` that the agent will use\n\nThe `AgentExecutor` class runs the agent. It expects the following parameters:\n\n* The `agent` to run\n* The `tools` that the agent can use\n* The `memory` which will store the conversation history", "The `AgentExecutor` class runs the agent. It expects the following parameters:\n\n* The `agent` to run\n* The `tools` that the agent can use\n* The `memory` which will store the conversation history\n\n[TIP]\n.AgentExecutor parameters\n====\nYou may find the following additional parameters useful when initializing an agent:\n\n* `max_iterations` - the maximum number of iterations to run the LLM for. This is useful in preventing the LLM from running for too long or entering an infinite loop.\n* `verbose` - if `True` the agent will print out the LLM output and the tool output.\n* `handle_parsing_errors` - if `True` the agent will handle parsing errors and return a message to the user.\n\n[source, python]\n----\nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    memory=memory,\n    max_interations=3,\n    verbose=True,\n    handle_parse_errors=True\n)\n----\n====\n\n=== Multiple tools\n\nA key advantage of using an agent is that they can use multiple tools. Access to multiple tools allows you to create agents that can perform several specific tasks.\n\nYou can extend this example to allow it to search YouTube for movie trailers by adding the link:https://python.langchain.com/docs/integrations/tools/youtube/[YouTubeSearchTool^] to the `tools` list.\n\nFirsly, you will need to install the link:https://pypi.org/project/youtube-search/[`youtube-search`^] package.\n\n[source, bash]\n----\npip install youtube-search\n----\n\nImport the `YouTubeSearchTool` and create a new tool.", "[source, bash]\n----\npip install youtube-search\n----\n\nImport the `YouTubeSearchTool` and create a new tool.\n\n[source, python]\n----\nfrom langchain_community.tools import YouTubeSearchTool\n\nyoutube = YouTubeSearchTool()\n\ntools = [\n    Tool.from_function(\n        name=\"Movie Chat\",\n        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n        func=chat_chain.run,\n        return_direct=True\n    ),\n    Tool.from_function(\n        name=\"Movie Trailer Search\",\n        description=\"Use when needing to find a movie trailer. The question will include the word 'trailer'. Return a link to a YouTube video.\",\n        func=youtube.run,\n        return_direct=True\n    )\n]\n----\n\n[%collapsible]\n.Click here to reveal the complete program\n====\n[source, python]\n----\ninclude::code/movie-trailer-agent.py[]\n----\n====\n\nThe model will then use the `name` and `description` for each tool to decide which tool to use.\n\nWhen prompted to find a movie trailer, the model should use the `YouTubeSearchTool` tool.\n\n    [user] Find the movie trailer for the Matrix.\n\n    [agent] ['https://www.youtube.com/watch?v=vKQi3bBA1y8&pp=ygUUTWF0cml4IG1vdmllIHRyYWlsZXI%3D', 'https://www.youtube.com/watch?v=9ix7TUGVYIo&pp=ygUUTWF0cml4IG1vdmllIHRyYWlsZXI%3D']\n\nHowever, when asked about movies, genres or plots, the model will use the `chat_chain` tool.\n\n    [user] Find a movie about the meaning of life", "However, when asked about movies, genres or plots, the model will use the `chat_chain` tool.\n\n    [user] Find a movie about the meaning of life\n\n    [agent] Certainly! One movie that explores the meaning of life is \"The Tree of Life\" directed by Terrence Malick. It follows the journey of a young boy as he grows up in the 1950s and reflects on his experiences and the meaning of existence. It's a visually stunning and thought-provoking film that delves into existential questions.\n\nAs the agent also uses the conversation memory you can refer back to the previous questions, such as finding a trailer for a movie it has recommended:\n\n    [user] Can you find the trailer\n\n    [agent] ['https://www.youtube.com/watch?v=RrAz1YLh8nY&pp=ygUeVGhlIFRyZWUgb2YgTGlmZSBtb3ZpZSB0cmFpbGVy', 'https://www.youtube.com/watch?v=OKqqboXuvyE&pp=ygUeVGhlIFRyZWUgb2YgTGlmZSBtb3ZpZSB0cmFpbGVy']\n\nAgents and tools allow you to create more adaptable and flexible models to perform multiple tasks.\n\n== Check Your Understanding\n\ninclude::questions/1-agents.adoc[leveloffset=+1]\n\n[.summary]\n== Summary\n\nIn this lesson, you learned how to create an agent to use multiple tools.\n\nIn the next lesson, you will learn how to use Langchain to connect to a Neo4j database.", "= Connecting to Neo4j\n:order: 7\n:type: lesson\n:disable-cache: true\n\nLangchain includes functionality to integrate directly with Neo4j, including allowing you to run Cypher statements, query vector indexes and use Neo4j as a vector store.\n\nIn this lesson, you will learn how to connect to a Neo4j instance and run Cypher statement.\n\n== Connecting to a Neo4j instance\n\nYou must install the `neo4j` Python package to connect to a Neo4j database.\n\n[source,shell]\n----\npip install neo4j\n----\n\nThe following code will connect to a Neo4j database and run a simple query.\n\n[source,python]\n----\ninclude::code/connect-to-neo4j.py[]\n----\n\nYou can connect to the Neo4j sandbox created for you when you joined the course.\n\nUpdate the code above to use the `url`, `username` and `password` of your Neo4j sandbox.\n\nConnection URL:: [copy]#bolt://{sandbox_ip}:{sandbox_boltPort}#\nUsername:: [copy]#{sandbox_username}#\nPassword:: [copy]#{sandbox_password}#\n\nRun the query - you should see data about the movie Toy Story.\n\nThe `Neo4jGraph` class is a wrapper to the link:https://neo4j.com/docs/python-manual/current/[Neo4j Python driver^]. It simplifies connecting to Neo4j and integrating with the Langchain framework.\n\n== Schema\n\nWhen you connect to the Neo4j database, the object loads the database schema into memory - this enables Langchain to access the schema information without having to query the database.\n\nYou can access the schema information using the `schema` property.", "You can access the schema information using the `schema` property.\n\n[source,python]\n----\nprint(graph.schema)\n----", "[%collapsible]\n.View schema\n====\n    Node properties are the following:\n    [{'properties': [{'property': 'url', 'type': 'STRING'}, {'property': 'runtime', 'type': 'INTEGER'}, {'property': 'revenue', 'type': 'INTEGER'}, {'property': 'budget', 'type': 'INTEGER'}, {'property': 'imdbRating', 'type': 'FLOAT'}, {'property': 'released', 'type': 'STRING'}, {'property': 'countries', 'type': 'LIST'}, {'property': 'languages', 'type': 'LIST'}, {'property': 'plot', 'type': 'STRING'}, {'property': 'imdbVotes', 'type': 'INTEGER'}, {'property': 'imdbId', 'type': 'STRING'}, {'property': 'year', 'type': 'INTEGER'}, {'property': 'poster', 'type': 'STRING'}, {'property': 'movieId', 'type': 'STRING'}, {'property': 'tmdbId', 'type': 'STRING'}, {'property': 'title', 'type': 'STRING'}], 'labels': 'Movie'}, {'properties': [{'property': 'name', 'type': 'STRING'}], 'labels': 'Genre'}, {'properties': [{'property': 'userId', 'type': 'STRING'}, {'property': 'name', 'type': 'STRING'}], 'labels': 'User'}, {'properties': [{'property': 'url', 'type': 'STRING'}, {'property': 'name', 'type': 'STRING'}, {'property': 'tmdbId', 'type': 'STRING'}, {'property': 'bornIn', 'type': 'STRING'}, {'property': 'bio', 'type': 'STRING'}, {'property': 'died', 'type': 'DATE'}, {'property': 'born', 'type': 'DATE'}, {'property': 'imdbId', 'type': 'STRING'}, {'property': 'poster', 'type': 'STRING'}], 'labels': 'Actor'}, {'properties': [{'property': 'url', 'type': 'STRING'}, {'property': 'bornIn', 'type': 'STRING'}, {'property': 'born', 'type': 'DATE'}, {'property': 'died', 'type': 'DATE'}, {'property': 'tmdbId', 'type': 'STRING'}, {'property': 'imdbId', 'type': 'STRING'}, {'property': 'name', 'type': 'STRING'}, {'property': 'poster', 'type': 'STRING'}, {'property': 'bio', 'type': 'STRING'}], 'labels': 'Director'}, {'properties': [{'property': 'url', 'type': 'STRING'}, {'property': 'bornIn', 'type': 'STRING'}, {'property': 'bio', 'type': 'STRING'}, {'property': 'died', 'type': 'DATE'}, {'property': 'born', 'type': 'DATE'}, {'property': 'imdbId', 'type': 'STRING'}, {'property': 'name', 'type': 'STRING'}, {'property': 'poster', 'type': 'STRING'}, {'property': 'tmdbId', 'type': 'STRING'}], 'labels': 'Person'}]\n    Relationship properties are the following:\n    [{'type': 'RATED', 'properties': [{'property': 'rating', 'type': 'FLOAT'}, {'property': 'timestamp', 'type': 'INTEGER'}]}, {'type': 'ACTED_IN', 'properties': [{'property': 'role', 'type': 'STRING'}]}, {'type': 'DIRECTED', 'properties': [{'property': 'role', 'type': 'STRING'}]}]\n    The relationships are the following:\n    ['(:Movie)-[:IN_GENRE]->(:Genre)', '(:User)-[:RATED]->(:Movie)', '(:Actor)-[:ACTED_IN]->(:Movie)', '(:Actor)-[:DIRECTED]->(:Movie)', '(:Director)-[:DIRECTED]->(:Movie)', '(:Director)-[:ACTED_IN]->(:Movie)', '(:Person)-[:DIRECTED]->(:Movie)', '(:Person)-[:ACTED_IN]->(:Movie)']\n====", "[TIP]\n.Refreshing the schema\nYou can refresh the schema by calling the `graph.refresh_schema()` method.\n\n== Check Your Understanding\n\ninclude::questions/1-neo4jgraph.adoc[leveloffset=+1]\n\n[.summary]\n== Summary\n\nIn this lesson, you learned how to connect to a Neo4j database and run Cypher statements.\n\nIn the next lesson, you will learn how to use Neo4j as a vector store using Langchain **Receivers**.", "= Retrievers\n:order: 8\n:type: lesson\n:disable-cache: true\n\nlink:https://python.langchain.com/docs/modules/data_connection/retrievers/[Retrievers^] are Langchain chain components that allow you to retrieve documents using an unstructured query.\n\n    Find a movie plot about a robot that wants to be human.\n\nDocuments are any unstructured text that you want to retrieve. A retriever often uses a vector store as its underlying data structure.\n\nRetrievers are a key component for creating models that can take advantage of Retrieval Augmented Generation (RAG).\n\nPreviously, you loaded embeddings and created a vector index of Movie plots - in this example, the movie plots are the _documents_, and a _retriever_ could be used to give a model context.\n\nIn this lesson, you will create a _retriever_ to retrieve documents from the movie plots vector index.\n\n== Neo4jVector\n\nThe link:https://python.langchain.com/docs/integrations/vectorstores/neo4jvector[`Neo4jVector`^] is a Langchain vector store that uses a Neo4j database as the underlying data structure.\n\nYou can use the `Neo4jVector` to generate embeddings, store them in the database and retrieve them.\n\n=== Querying a vector index\n\nReview the following code that creates a `Neo4jVector` from the `moviePlots` index you created.\n\n[source,python]\n----\ninclude::code/query_vector.py[]\n----\n\nYou should be able to identify the following:", "[source,python]\n----\ninclude::code/query_vector.py[]\n----\n\nYou should be able to identify the following:\n\n* That an `embedding_provider` is required. In this case, `OpenAIEmbeddings`, as this was used to originally create the embeddings. The embedding provider will be used to generate embeddings for any queries.\n* The connection details for the Neo4j database (`url`, `username`, `password`).\n* The name of the Neo4j index (`\"moviePlots\"`).\n* The name of the node property that contains the embeddings (`\"embedding\"`).\n* The name of the node property that contains the text (`\"plot\"`).\n* The `similarity_search()` method is used to retrieve documents. The first argument is the query.\n\nTo run this program you will need to:\n\n. Replace the `openai_api_key` with your OpenAI API key\n. Update Neo4j connection details with your Sandbox connection details.\n+\n[%collapsible]\n.Click to reveal your Sandbox connection details\n====\nYour Neo4j Sandbox connection details are:\n\nConnection URL:: [copy]#bolt://{sandbox_ip}:{sandbox_boltPort}#\nUsername:: [copy]#{sandbox_username}#\nPassword:: [copy]#{sandbox_password}#\n====\n\nRun the code and review the results. Try different queries and see what results you get.\n\nThe `similarity_search()` method returns a list of link:https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html[`Document`^] objects. The `Document` object includes the properties:", "* `page_content` - the content referenced by the index, in this example the plot of the movie\n* `meta_data` - a dictionary of the `Movie` node properties returned by the index\n\n[TIP]\n.Specify the number of documents\n====\nYou can pass an optional `k` argument to the `similarity_search()` method to specify the number of documents to return. The default is 4.\n\n[source,python]\n----\nvector.similarity_search(query, k=1)\n----\n====\n\n=== Creating a new vector index\n\nThe `Neo4jVector` class can also generate embeddings and vector indexes - this is useful when creating vectors programmatically or at run time.\n\nThe following code would create embeddings and a new index called `myVectorIndex` in the database for `Chunk` nodes with a `text` property:\n\n[source,python]\n----\ninclude::code/create_index.py[]\n----\n\n== Creating a Retriever chain\n\nTo incorporate a retriever and Neo4j vector into a Langchain application, you can create a _retrieval_ chain.\n\nThe `Neo4jVector` class has a `as_retriever()` method that returns a retriever.\n\nThe link:https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html[`RetrievalQA`^] class is a chain that uses a retriever as part of its pipeline. It will use the retriever to retrieve documents and pass them to a language model.\n\nBy incorporating `Neo4jVector` into a `RetrievalQA` chain, you can use data and vectors in Neo4j in a Langchain application.", "By incorporating `Neo4jVector` into a `RetrievalQA` chain, you can use data and vectors in Neo4j in a Langchain application.\n\nReview this program incorporating the `moviePlots` vector index into a retrieval chain.\n\n[source,python]\n----\ninclude::code/retreiver_chain.py[]\n----\n\nWhen the program runs, the `RetrievalQA` chain will use the `movie_plot_vector` retriever to retrieve documents from the `moviePlots` index and pass them to the `chat_llm` language model.\n\n[TIP]\n.Understanding the results\n====\nIt can be difficult to understand how the model generated the response and how the retriever affected it.\n\nBy setting the optional `verbose` and `return_source_documents` arguments to `True` when creating the `RetrievalQA` chain, you can see the source documents and the retriever's score for each document.\n\n[source, python]\n----\nplot_retriever = RetrievalQA.from_llm(\n    llm=chat_llm,\n    retriever=movie_plot_vector.as_retriever(),\n    verbose=True,\n    return_source_documents=True\n)\n----\n====\n\nRetrievers and vector indexes allow you to incorporate unstructured data into your Langchain applications.\n\n== Check Your Understanding\n\ninclude::questions/1-retrievers.adoc[leveloffset=+1]\n\n[.summary]\n== Summary\n\nIn this lesson, you learned how to incorporate Neo4j vector indexes and retrievers into Langchain applications.\n\nIn the next optional challenge, you will add the movie plots vector retriever to the chat agent you created in the previous lesson.", "= Adding the Neo4j Vector Retriever\n:order: 9\n:type: challenge\n:optional: true\n\nIn this *optional* challenge, add the movie plots vector retriever to the movie trailer agent you created previously.\n\nTo complete the code you will need to update the movie trailer agent to:\n\n. Create the `Neo4jVector` from the `moviePlots` vector index.\n. Create the `RetrievalQA` chain using the `Neo4jVector` as the retriever.\n. Update the `tools` to use the `RetrievalQA` chain.\n\n[%collapsible]\n.Here is the code for the movie trailer agent\n====\n[source, python]\n----\ninclude::../4-agents/code/movie-trailer-agent.py[]\n----\n====\n\n[%collapsible]\n.Here is the code for the movie plots vector retriever\n====\n[source, python]\n----\ninclude::../6-retrievers/code/retreiver_chain.py[]\n----\n====\n\n[TIP]\n.Running a RetrievalQA chain from a tool\n====\nTools expect a single `query` input and a single output key.\n\nThe `RetrievalQA` chain returns multiple output keys.\n\nAs a result, the agent's tool executor cannot call the `RetrievalQA` chain directly e.g. using `func=retrievalQA.run`.\n\nYou could wrap the `RetrievalQA` chain in a function that takes a single string input, format the results and return a single string.\n\n[source,python]\n----\n# Create a function to invoke the retriever\ndef run_retriever(query):\n    results = plot_retriever.invoke({\"query\":query})\n    return str(results)", "[source,python]\n----\n# Create a function to invoke the retriever\ndef run_retriever(query):\n    results = plot_retriever.invoke({\"query\":query})\n    return str(results)\n\n\n# Append the tool to the tools array\ntools = [\n    # ...\n    Tool.from_function(\n        ...\n        func=run_retriever,\n        ...\n    )\n]\n----\n====\n\n[%collapsible]\n.Click to reveal the solution\n====\nThere is no right or wrong way to complete this challenge. Here is one potential solution.\n\n[source, python]\n----\ninclude::code/retriever_agent.py[]\n----\n====\n\n== Summary\n\nIn this optional challenge, you added the movie plots vector retriever to the movie trailer agent you created previously.\n\nIn the next module, you will learn how to use an LLM to generate Cypher and improve the responses of an LLM.\n\nread::Continue[]\n\n[.summary]\n== Summary\n\nIn this optional challenge, you added the movie plots vector retriever to the movie trailer agent you created previously.\n\nIn the next module, you will learn how to use an LLM to generate Cypher and improve the responses of an LLM.", "= The Cypher QA Chain\n:order: 1\n:type: lesson\n:disable-cache: true\n\n// https://adamcowley.co.uk/posts/abridged-neo4j-cypher-generation/\n\nLanguage models and vector indexes are good at querying unstructured data. Although, as you have seen, responses are not always correct, and when data is structured, it is often easier to query it directly.\n\nLLMs are good at writing Cypher queries when given good information, such as:\n\n* The schema of the graph\n* Context about the question to be answered\n* Examples of questions and appropriate Cypher queries\n\nIn this lesson, you will learn how to use a language model to generate Cypher queries to query a Neo4j graph database.\n\n== Generating Cypher\n\nLangchain includes the link:https://api.python.langchain.com/en/latest/_modules/langchain/chains/graph_qa/cypher.html#GraphCypherQAChain[`GraphCypherQAChain`^]chain that can interact with a Neo4j graph database. It uses a language model to generate Cypher queries and then uses the graph to answer the question.\n\n`GraphCypherQAChain` chain requires the following:\n\n* An LLM (`llm`) for generating Cypher queries\n* A graph database connection (`graph`) for answering the queries\n* A prompt template (`cypher_prompt`) to give the LLM the schema and question\n* An appropriate question which relates to the schema and data in the graph\n\nThe program below will generate a Cypher query based on the schema in the graph database and the question.\n\nReview the code and predict what will happen when you run it.", "The program below will generate a Cypher query based on the schema in the graph database and the question.\n\nReview the code and predict what will happen when you run it.\n\n[source,python]\n----\ninclude::code/cypher-gen.py[]\n----\n\n[NOTE]\nBefore running the program, you must update the `openai_api_key` and the `graph` connection details.\n\n[%collapsible]\n.Click to reveal your Sandbox connection details\n====\nYour Neo4j Sandbox connection details are:\n\nConnection URL:: [copy]#bolt://{sandbox_ip}:{sandbox_boltPort}#\nUsername:: [copy]#{sandbox_username}#\nPassword:: [copy]#{sandbox_password}#\n====\n\nWhen you run the program, you should see the Cypher generated from the question and the data it returned. Something similar to:\n\n    Generated Cypher:\n    MATCH (a:Actor)-[r:ACTED_IN]->(m:Movie {title: 'Toy Story'})\n    WHERE a.name = 'Tom Hanks'\n    RETURN r.role\n\n    Full Context:\n    [{'r.role': 'Woody (voice)'}]\n\nThe LLM used the database schema to generate an _appropriate_ Cypher query. Langchain then executed the query against the graph database, and the result returned.\n\n== Breaking Down the Program\n\nReviewing the program, you should identify the following key points:\n\n. The program instantiates the required `llm` and `graph` objects using the appropriate API and connection details.\n+\n[source,python]\n----\nllm = ChatOpenAI(\n    openai_api_key=\"sk-...\"\n)", ". The program instantiates the required `llm` and `graph` objects using the appropriate API and connection details.\n+\n[source,python]\n----\nllm = ChatOpenAI(\n    openai_api_key=\"sk-...\"\n)\n\ngraph = Neo4jGraph(\n    url=\"bolt://localhost:7687\",\n    username=\"neo4j\",\n    password=\"pleaseletmein\",\n)\n----\n. The `CYPHER_GENERATION_TEMPLATE` gives the LLM context. The schema and question are passed to the LLM as input variables.\n+\n[source,python]\n----\nCYPHER_GENERATION_TEMPLATE = \"\"\"\nYou are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\nConvert the user's question based on the schema.\n\nSchema: {schema}\nQuestion: {question}\n\"\"\"", "Schema: {schema}\nQuestion: {question}\n\"\"\"\n\ncypher_generation_prompt = PromptTemplate(\n    template=CYPHER_GENERATION_TEMPLATE,\n    input_variables=[\"schema\", \"question\"],\n)\n----\n+\nThe `schema` will be automatically generated from the graph database and passed to the LLM. The `question` will be the user's question.\n. The program instantiates the `GraphCypherQAChain` chain with the `llm`, `graph`, and prompt template (`cypher_prompt`).\n+\n[source,python]\n----\ncypher_chain = GraphCypherQAChain.from_llm(\n    llm,\n    graph=graph,\n    cypher_prompt=cypher_generation_prompt,\n    verbose=True\n)\n----\n+\nThe program sets the `verbose` flag to `True` so you can see the generated Cypher query and response.\n. The chain runs, passing an appropriate question.\n+\n[source,python]\n----\ncypher_chain.invoke({\"query\": \"What role did Tom Hanks play in Toy Story?\"})\n----\n\nExperiment with different questions and observe the results.\n\nFor example, try:\n\n. A different context - \"What movies did Meg Ryan act in?\"\n. An aggregate query - \"How many movies has Tom Hanks directed?\"\n\n== Inconsistent Results\n\nInvestigate what happens when you ask the same question multiple times. Observe the generated Cypher query and the response.\n\n    \"What role did Tom Hanks play in Toy Story?\"\n\nYou will likely see different results each time you run the program.", "\"What role did Tom Hanks play in Toy Story?\"\n\nYou will likely see different results each time you run the program.\n\n    MATCH (actor:Actor {name: 'Tom Hanks'})-[:ACTED_IN]->(movie:Movie {title: 'Toy Story'})\n    RETURN actor.name, movie.title, movie.year, movie.runtime, movie.plot\n\n    MATCH (a:Actor {name: 'Tom Hanks'})-[:ACTED_IN]->(m:Movie {title: 'Toy Story'})-[:ACTED_IN]->(p:Person)\n    RETURN p.name AS role\n\nThe LLM doesn't return consistent results - its objective is to produce an answer, not the same response, and they may not be correct.\n\nYou will see similar problems when you ask the LLM different questions.\n\n[source,python]\n----\ncypher_chain.invoke({\"query\": \"What movies has Tom Hanks acted in?\"})\ncypher_chain.invoke({\"query\": \"How many movies has Tom Hanks directed?\"})\n----\n\nIn the following two lessons, you will learn how to provide additional context and instructions to the LLM to generate better and more consistent results.\n\n== Check Your Understanding\n\ninclude::questions/1-cypher-chain.adoc[leveloffset=+1]\n\n\n[.summary]\n== Summary\n\nIn this lesson, you learned how to use a language model to generate Cypher queries.\n\nIn the next lesson, you will experiment with different prompts to improve the results.", "= Providing Specific Instructions\n:order: 2\n:type: lesson\n\nIn this lesson, you will learn how to provide instructions to the LLM to improve the responses to questions.\n\nYou provide instructions and context to the LLM in the prompt.\n\n== Following the schema\n\nIn the previous lesson, you used this prompt to generate Cypher statements:\n\n[source,python]\n----\nCYPHER_GENERATION_TEMPLATE = \"\"\"\nYou are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\nConvert the user's question based on the schema.\n\nSchema: {schema}\nQuestion: {question}\n\"\"\"\n----\n\nThe LLM's training data included many Cypher statements, but these statements were not specific to the structure of your graph database. As a result the LLM may generate Cypher statements that are not valid and do not conform to the schema. \n\nYou can provide specific instructions to the LLM to state that the generated Cypher statements should follow the schema.\n\nFor example, you could give instructions only to use the provided relationship types and properties in the schema.\n\n[source,python]\n----\nCYPHER_GENERATION_TEMPLATE = \"\"\"\nYou are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\nConvert the user's question based on the schema.", "Instructions:\nUse only the provided relationship types and properties in the schema.\nDo not use any other relationship types or properties that are not provided.\n\nSchema: {schema}\nQuestion: {question}\n\"\"\"\n----\n\n[TIP]\n.Instructions not rules\nThere is no right or wrong way to provide instructions to the LLM. The instructions are unlike traditional programming tasks where you would supply rules or definitions. You may have to experiment to find the right balance between providing too much or too little instruction.\n\nFor example, the instructions are expressed as both a positive and negative statement. You will do this. You won't do this. \n\n    Use only the provided relationship types and properties in the schema.\n    Do not use any other relationship types or properties that are not provided.\n\nThe results of this change may not be immediately apparent when you run the program. Instructions allows you to fine-tune the LLM's responses and make them more consistent.\n\n== Understanding data\n\nThe LLM may also need additional instructions about the data. For example, how data is formatted or how to handle missing data.\n\nIf you ask the LLM to generate Cypher for the question:\n    \n    Who acted in The Matrix and what roles did they play?\n\nThe LLM will generate the correct Cypher:\n\n    MATCH (m:Movie {title: 'The Matrix'})<-[:ACTED_IN]-(a:Actor)\n    RETURN a.name as Actor, a.role as Role", "The LLM will generate the correct Cypher:\n\n    MATCH (m:Movie {title: 'The Matrix'})<-[:ACTED_IN]-(a:Actor)\n    RETURN a.name as Actor, a.role as Role\n\nHowever, the graph database returns no data because movies that start with `The` are renamed as `{title}, The`. `The Matrix` is stored as `Matrix, The`.\n\nYou can provide instructions to the LLM to fix this problem.\n\n    For movie titles that begin with \"The\", move \"the\" to the end, For example \"The 39 Steps\" becomes \"39 Steps, The\" or \"The Matrix\" becomes \"Matrix, The\".\n\n[source, python]\n----\nCYPHER_GENERATION_TEMPLATE = \"\"\"\nYou are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\nConvert the user's question based on the schema.\n\nInstructions:\nUse only the provided relationship types and properties in the schema.\nDo not use any other relationship types or properties that are not provided.\nFor movie titles that begin with \"The\", move \"the\" to the end, For example \"The 39 Steps\" becomes \"39 Steps, The\" or \"The Matrix\" becomes \"Matrix, The\".\n\nSchema: {schema}\nQuestion: {question}\n\"\"\"\n----\n\nIf you ask the LLM the same question, it should now format the title correctly and return the correct data:\n\n    MATCH (m:Movie {title: 'Matrix, The'})<-[:ACTED_IN]-(a:Actor)\n    RETURN a.name as Actor, a.role as Role\n\n== Controlling the response\n\nAs well as instructing the LLM on how to deal with the question, you can also instruct the LLM on how to respond.", "== Controlling the response\n\nAs well as instructing the LLM on how to deal with the question, you can also instruct the LLM on how to respond.\n\nYou could instruct the LLM only to respond when the Cypher statement returns data.\n\n    If no data is returned, do not attempt to answer the question.\n\nYou may want the LLM to only respond to questions in the scope of the task. For example:\n\n    Only respond to questions that require you to construct a Cypher statement.\n\n    Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n\nConcise responses from the LLM may be needed:\n\n    Do not include any explanations or apologies in your responses.\n\nOr you may want to restrict the format of the response:\n\n    Do not include any text except the generated Cypher statement.\n\nUltimately, you must fine-tune your instructions for the specific task to ensure the best results.\n\n== Check Your Understanding\n\ninclude::questions/1-instructions.adoc[leveloffset=+1]\n\n[.summary]\n== Summary\n\nIn this lesson, you learned something how to provide the LLM with instructions to improve the responses to questions.\n\nIn the next lesson, you will learn about few-shot examples and how they can improve the LLM's performance.", "= Few-shot Examples\n:order: 3\n:type: lesson\n\nEven though you have provided an LLM with specific instructions, it can still make mistakes. \n\nIn this lesson, you will learn about Few-Shot examples and how to use them to improve the performance of the LLM.\n\nlink:https://www.promptingguide.ai/techniques/fewshot[Few-Shot Prompting^] is a technique where you provide the LLM with an example of how to respond or generate a response to a specific scenario.\n\n== Example Cypher\n\nHere is a use case demonstrating a problem that would benefit from Few-Shot prompting.\n\nYou ask the LLM to generate a Cypher statement to answer the question:\n\n    What movies has Tom Hanks directed and what are the genres?\n\nThe LLM generates the following Cypher statement:\n\n    MATCH (p:Person)-[:DIRECTED]->(m:Movie) WHERE p.name = 'Tom Hanks' RETURN m.genres\n\nThe Cypher statement returns the following result:\n\n    [{'genres': null}, {'genres': null}]\n\nFrom this data, the LLM can understand that Tom Hanks has directed two movies, but there is no information about the genres of those movies.\n\nThe generated Cypher statement is wrong because it uses the `m.genres` property, which doesn't exist. Instead, it should follow the `:IN_GENRE` relationship to `:Genre` nodes and use the `.name` property.\n\nYou can improve the LLM by providing an example of a correct Cypher statement in the Cypher generation prompt:\n\n    Examples: \n\n    Find movies and genres:\n    MATCH (m:Movie)-[:IN_GENRE]->(g)\n    RETURN m.title, g.name", "Examples: \n\n    Find movies and genres:\n    MATCH (m:Movie)-[:IN_GENRE]->(g)\n    RETURN m.title, g.name\n\n[%collapsible]\n.Click to reveal the full Cypher generation prompt\n====\n[source,python]\n----\nCYPHER_GENERATION_TEMPLATE = \"\"\"\nYou are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\nConvert the user's question based on the schema.\n\nInstructions:\nUse only the provided relationship types and properties in the schema.\nDo not use any other relationship types or properties that are not provided.\nFor movie titles that begin with \"The\", move \"the\" to the end, For example \"The 39 Steps\" becomes \"39 Steps, The\" or \"The Matrix\" becomes \"Matrix, The\".\n\nIf no data is returned, do not attempt to answer the question.\nOnly respond to questions that require you to construct a Cypher statement.\nDo not include any explanations or apologies in your responses.\n\nExamples: \n\nFind movies and genres:\nMATCH (m:Movie)-[:IN_GENRE]->(g)\nRETURN m.title, g.name\n\nSchema: {schema}\nQuestion: {question}\n\"\"\"\n----\n====\n\nThe LLM can use the example to help it generate the correct Cypher statement:\n\n    MATCH (p:Person)-[:DIRECTED]->(m:Movie)-[:IN_GENRE]->(g:Genre)\n    WHERE p.name = 'Tom Hanks'\n    RETURN DISTINCT g.name\n\nWhich returns the correct data:\n\n    [{'g.name': 'Drama'}, {'g.name': 'Comedy'}, {'g.name': 'Romance'}]", "Which returns the correct data:\n\n    [{'g.name': 'Drama'}, {'g.name': 'Comedy'}, {'g.name': 'Romance'}]\n\n[NOTE]\n====\nThe few-shot example is not the complete Cypher statement to answer the question, but it is enough to show the LLM how to use the `[:IN_GENRE]` relationship.\n====\n\nThe LLM can now also generate the correct Cypher statement for other questions involving movies and genres:\n\n    What genre of film is Toy Story?\n\n    MATCH (m:Movie {title: 'Toy Story'})-[:IN_GENRE]->(g:Genre)\n    RETURN g.name\n\n    [{'g.name': 'Adventure'}, {'g.name': 'Animation'}, {'g.name': 'Children'}, {'g.name': 'Comedy'}, {'g.name': 'Fantasy'}]\n\nFew-shot prompts allow you to provide targeted examples to the LLM to improve its performance. Providing examples can improve the LLM's performance on a specific task or the performance in general. \n\n== Check Your Understanding\n\ninclude::questions/1-few-shot-prompts.adoc[leveloffset=+1]\n\n[.summary]\n== Summary\n\nIn this lesson, you learned about Few-Shot prompting and how to use it to improve the performance of the LLM.\n\nIn the next optional challenge, you will add the Cypher generation chain to an agent and give it conversation memory.", "= Conversational Agent\n:order: 4\n:type: challenge\n:optional: true\n\nDuring this course, you have learned about the fundamentals of LLMs, how to interact with them using langchain, and how Neo4j can support the LLM in answering questions.\n\nIn this optional challenge you will apply this new knowledge and skills to a new problem.\n\nYou will create a conversational agent using Langchain that can answer questions about a topic you are interested in.\n\nThe agent should:\n\n* Use a prompt that sets the scene for the LLM\n* Be given additional context that is relevant to the scenario\n* Have conversational memory and be able to answer a string of questions\n* Be given a few-shot example to support it in answering a specific type of question\n\nI recommend that you approach this challenge in the following order:\n\n. Choose a topic that you are interested in\n. Create a prompt and create a chain that uses the prompt\n. Create an agent and add conversational memory\n. Add additional context to the agent\n. Add a few-shot example to the agent\n\n== Next Steps\n\nYou can learn more about creating chat bots using Neo4j in the next GraphAcademy course - link:https://graphacademy.neo4j.com/courses/llm-chatbot-python/[Build a Neo4j-backed Chatbot using Python^].\n\nread::Continue[]\n\n[.summary]\n== Summary\n\nIn this optional challenge, you used the knowledge and skills you have learned during this course to create a conversational agent.", "read::Continue[]\n\n[.summary]\n== Summary\n\nIn this optional challenge, you used the knowledge and skills you have learned during this course to create a conversational agent.\n\nCongratulations on completing this course. I hope you have enjoyed it and learned a lot."]